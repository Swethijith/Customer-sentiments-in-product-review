{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbe739be-47ad-4ed7-b1d8-07f22b99c97b",
   "metadata": {},
   "source": [
    "# Data module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a63817-3c92-47d5-86b6-df6d4faefc46",
   "metadata": {},
   "source": [
    "## dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29924ca-cfdc-4734-9241-dc0b5eea7cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from icecream import ic\n",
    "from data import data_dir_path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "def split_dataframe(df):\n",
    "    \"\"\"\n",
    "    Split a dataframe into train, test, and validation sets.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The dataframe to be split.\n",
    "    train_size (float): Proportion of the dataset to include in the train split.\n",
    "    test_size (float): Proportion of the dataset to include in the test split.\n",
    "    val_size (float): Proportion of the dataset to include in the validation split.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Tuple containing three dataframes (train, test, validation).\n",
    "    \"\"\"\n",
    "    train_size=0.7\n",
    "    test_size=0.2\n",
    "    val_size=0.1\n",
    "\n",
    "    # First, split into train and temp (test + validation)\n",
    "    train_df, temp_df = train_test_split(df, train_size=train_size)\n",
    "\n",
    "    # Calculate the proportion of temp_df to be used for test to maintain overall test_size proportion\n",
    "    proportion_of_temp_for_test = test_size / (test_size + val_size)\n",
    "\n",
    "    # Split temp into test and validation\n",
    "    test_df, val_df = train_test_split(temp_df, train_size=proportion_of_temp_for_test)\n",
    "\n",
    "    return train_df, test_df, val_df\n",
    "\n",
    "def balance_data(df:pd.DataFrame)->pd.DataFrame:\n",
    "\n",
    "    # Min count of sentiment category\n",
    "    min_count = df['sentiment'].value_counts().min()\n",
    "    # min_count = 10000\n",
    "\n",
    "    # Create a new DataFrame to store the balanced data\n",
    "    balanced_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate through each category\n",
    "    for category in df['sentiment'].unique():\n",
    "        # Randomly sample 'min_count' reviews from each category\n",
    "        sampled_reviews = df[df['sentiment'] == category].sample(min_count, random_state=42)\n",
    "        \n",
    "        # Append the sampled reviews to the balanced DataFrame\n",
    "        balanced_df = pd.concat([balanced_df, sampled_reviews], ignore_index=True)\n",
    "    \n",
    "    return balanced_df\n",
    "\n",
    "def get_sentiment_from_rating(rating:float)->str:\n",
    "    \"\"\"Convert rating to sentiments\"\"\"\n",
    "\n",
    "    rating = int(rating)\n",
    "    if rating < 3:\n",
    "        return \"negative\"\n",
    "    elif rating > 3:\n",
    "        return \"positive\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "\n",
    "# Path of json file of fashion product reviews\n",
    "data_path = os.path.join(\n",
    "    data_dir_path,\n",
    "    \"fashion_data\\AMAZON_FASHION.json\"\n",
    ")\n",
    "\n",
    "reviews = list()\n",
    "with open(data_path,'r') as file:\n",
    "    for row in file:\n",
    "        reviews.append(json.loads(row))\n",
    "    review_data = pd.DataFrame(reviews)[['overall','reviewText']]\n",
    "\n",
    "ic(review_data.isna().sum())\n",
    "\n",
    "# Drop null values\n",
    "review_data.dropna(inplace=True)\n",
    "\n",
    "# Reset index\n",
    "review_data.reset_index(inplace=True, drop=True)\n",
    "\n",
    "review_data['sentiment'] = review_data['overall'].apply(get_sentiment_from_rating)\n",
    "ic(len(review_data), \"reviews loaded.\")\n",
    "\n",
    "balanced_review_data = balance_data(review_data)\n",
    "ic(len(balanced_review_data), \"reviews available.\")\n",
    "\n",
    "train_data, val_data, test_data = split_dataframe(balanced_review_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d94cd99-f208-4e5c-ad9e-27b1da375d33",
   "metadata": {},
   "source": [
    "# EDA module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcf8cb6-b144-4ff6-8aaa-fa2c8806a402",
   "metadata": {},
   "source": [
    "## data_exploration.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f6462b-9592-4b66-860b-68939f49f8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_review_sentiment_count_bars(\n",
    "        value_counts:pd.Series,\n",
    "        save_folder: str, \n",
    "        save_name:str,\n",
    "        chart_title:str\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Create a bar chart from value counts and save it to a given folder.\n",
    "\n",
    "    Parameters:\n",
    "        value_counts (pd.Series): Value counts data.\n",
    "        save_folder (str): Folder path where the chart will be saved.\n",
    "        chart_title (str, optional): Title for the bar chart.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame from the value counts series\n",
    "    df = pd.DataFrame({'Values': value_counts.index, 'Counts': value_counts.values})\n",
    "    \n",
    "    # Create the bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(df['Values'], df['Counts'])\n",
    "    plt.xlabel('Values')\n",
    "    plt.ylabel('Counts')\n",
    "    plt.title(chart_title)\n",
    "    plt.xticks(rotation=0)\n",
    "\n",
    "    # Add count labels at the top of each bar\n",
    "    for i, count in enumerate(df['Counts']):\n",
    "        plt.text(df['Values'][i], count, str(count), ha='center', va='bottom', fontsize=12)\n",
    "    \n",
    "    # Save the chart to the specified folder\n",
    "    save_path = f\"{save_folder}/{save_name}.png\"\n",
    "    plt.savefig(save_path, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df7655f-a133-4e62-85be-d9263df5b2ec",
   "metadata": {},
   "source": [
    "## main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356a9070-4ed0-4c13-9f4e-95b667f2f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from icecream import ic\n",
    "from eda.data_exploration import plot_review_sentiment_count_bars\n",
    "from data.dataloader import review_data, balanced_review_data\n",
    "from results import results_dir_path\n",
    "from utils.folder_utils import create_path\n",
    "\n",
    "\n",
    "# Review count bar plot\n",
    "save_folder = os.path.join(results_dir_path,\"eda\")\n",
    "create_path(save_folder)\n",
    "save_name = \"sentiments_counts\"\n",
    "plot_review_sentiment_count_bars(\n",
    "    value_counts=review_data.sentiment.value_counts(),\n",
    "    save_folder=save_folder,\n",
    "    save_name=save_name,\n",
    "    chart_title=\"Sentiments count bar plot\"\n",
    ")\n",
    "\n",
    "# Balanced review count bar plot\n",
    "save_folder = os.path.join(results_dir_path,\"eda\")\n",
    "create_path(save_folder)\n",
    "save_name = \"balanced_sentiments_counts\"\n",
    "plot_review_sentiment_count_bars(\n",
    "    value_counts=balanced_review_data.sentiment.value_counts(),\n",
    "    save_folder=save_folder,\n",
    "    save_name=save_name,\n",
    "    chart_title=\"Balanced data sentiments count bar plot\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc43be2b-5bab-41f4-9fd9-d31753ec7b4a",
   "metadata": {},
   "source": [
    "# Evaluate module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69a4d5b-9407-4117-b127-34a7a7302869",
   "metadata": {},
   "source": [
    "## metrics.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3f220d-1b96-43a6-9827-d81389dcea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    precision_score, \n",
    "    recall_score, \n",
    "    accuracy_score, \n",
    "    f1_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from icecream import ic\n",
    "from utils.folder_utils import create_path\n",
    "\n",
    "def calculate_correct_positive(actual, predicted):\n",
    "    total_actual_positive = actual.count('positive')\n",
    "    total_predicted_positive = predicted.count('positive')\n",
    "    \n",
    "    if total_actual_positive == 0:\n",
    "        return 0.00\n",
    "    \n",
    "    correct_positive = sum(1 for a, p in zip(actual, predicted) if a == 'positive' and p == 'positive')\n",
    "    percentage_correct_positive = (correct_positive / total_actual_positive) * 100\n",
    "    \n",
    "    return round(percentage_correct_positive, 2)\n",
    "\n",
    "def calculate_correct_negative(actual, predicted):\n",
    "    total_actual_negative = actual.count('negative')\n",
    "    total_predicted_negative = predicted.count('negative')\n",
    "    \n",
    "    if total_actual_negative == 0:\n",
    "        return 0.00\n",
    "    \n",
    "    correct_negative = sum(1 for a, p in zip(actual, predicted) if a == 'negative' and p == 'negative')\n",
    "    percentage_correct_negative = (correct_negative / total_actual_negative) * 100\n",
    "    \n",
    "    return round(percentage_correct_negative, 2)\n",
    "\n",
    "def calculate_positive_classified_as_negative(actual, predicted):\n",
    "    total_actual_positive = actual.count('positive')\n",
    "    \n",
    "    if total_actual_positive == 0:\n",
    "        return 0.00\n",
    "    \n",
    "    positive_classified_as_negative = sum(1 for a, p in zip(actual, predicted) if a == 'positive' and p == 'negative')\n",
    "    percentage_positive_classified_as_negative = (positive_classified_as_negative / total_actual_positive) * 100\n",
    "    \n",
    "    return round(percentage_positive_classified_as_negative, 2)\n",
    "\n",
    "def calculate_negative_classified_as_positive(actual, predicted):\n",
    "    total_actual_negative = actual.count('negative')\n",
    "    \n",
    "    if total_actual_negative == 0:\n",
    "        return 0.00\n",
    "    \n",
    "    negative_classified_as_positive = sum(1 for a, p in zip(actual, predicted) if a == 'negative' and p == 'positive')\n",
    "    percentage_negative_classified_as_positive = (negative_classified_as_positive / total_actual_negative) * 100\n",
    "    \n",
    "    return round(percentage_negative_classified_as_positive, 2)\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "        actual:list[str], \n",
    "        prediction:list[str], \n",
    "        save_file_path:str, \n",
    "        file_name:str\n",
    "    )->None:\n",
    "    \"\"\"\n",
    "    Evaluate sentiment analysis performance and write results to a text file.\n",
    "\n",
    "    Parameters:\n",
    "    actual (list): List of actual sentiment labels.\n",
    "    prediction (list): List of predicted sentiment labels.\n",
    "    save_file_path (str): Path to save the file.\n",
    "    file_name (str): Name of the file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculating metrics\n",
    "    precision = round(precision_score(actual, prediction, average='macro'), 2)\n",
    "    recall = round(recall_score(actual, prediction, average='macro'), 2)\n",
    "    accuracy = round(accuracy_score(actual, prediction), 2)\n",
    "    f1 = round(f1_score(actual, prediction, average='macro'), 2)\n",
    "    correct_positive_percentage = calculate_correct_positive(actual, prediction)\n",
    "    correct_negative_percentage = calculate_correct_negative(actual, prediction)\n",
    "    positive_classified_as_negative_percentage = calculate_positive_classified_as_negative(actual, prediction)\n",
    "    negative_classified_as_positive_percentage = calculate_negative_classified_as_positive(actual, prediction)\n",
    "\n",
    "    # Creating a DataFrame for the results\n",
    "    results_df = pd.DataFrame({\n",
    "        'Metric': [\n",
    "            'Precision', \n",
    "            'Recall', \n",
    "            'Accuracy', \n",
    "            'F1 Score',\n",
    "            'correct_positive_percentage',\n",
    "            'correct_negative_percentage',\n",
    "            'positive_classified_as_negative_percentage',\n",
    "            'negative_classified_as_positive_percentage'\n",
    "        ],\n",
    "        'Value': [\n",
    "            precision, \n",
    "            recall, \n",
    "            accuracy, \n",
    "            f1,\n",
    "            correct_positive_percentage,\n",
    "            correct_negative_percentage,\n",
    "            positive_classified_as_negative_percentage,\n",
    "            negative_classified_as_positive_percentage\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    ic(results_df)\n",
    "\n",
    "    create_path(save_file_path)\n",
    "\n",
    "    # Saving the results to a text file\n",
    "    full_path = f\"{save_file_path}/{file_name}.txt\"\n",
    "    with open(full_path, 'w') as file:\n",
    "        file.write(results_df.to_string(index=False))\n",
    "\n",
    "    return None\n",
    "\n",
    "def plot_and_save_confusion_matrix(actual, predicted, save_file_path, file_name):\n",
    "    \"\"\"\n",
    "    Calculate the confusion matrix, plot it using Matplotlib, and save the plot.\n",
    "\n",
    "    Parameters:\n",
    "    actual (list): List of actual labels.\n",
    "    predicted (list): List of predicted labels.\n",
    "    save_file_path (str): Path where the plot should be saved.\n",
    "    file_name (str): Name of the file to save the plot.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(actual, predicted, labels=[\"negative\", \"positive\", \"neutral\"])\n",
    "\n",
    "    # Plot using seaborn for a nicer looking heatmap\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='g', xticklabels=[\"negative\", \"positive\", \"neutral\"], yticklabels=[\"negative\", \"positive\", \"neutral\"])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "\n",
    "    # Save the plot\n",
    "    plt.savefig(f\"{save_file_path}/{file_name}.png\")\n",
    "\n",
    "    # Close the plot\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b0e5f-bb03-4b00-9cb8-63d40aced42f",
   "metadata": {},
   "source": [
    "# Utils module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58e0134-899a-4c5f-9a45-f6adc10f5a86",
   "metadata": {},
   "source": [
    "## folder_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb728d-245d-4661-85dc-c7c26f8a5126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_path(folder_path:str)->None:\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.mkdir(folder_path)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa09a07a-e6cd-40a7-b984-895a131221ff",
   "metadata": {},
   "source": [
    "# BERT module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60da5a75-fdb9-4d4a-a640-a512ac419a76",
   "metadata": {},
   "source": [
    "## training.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8ffb3a-2f82-4529-bc78-ac1999fe8c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from data.dataloader import train_data,test_data\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573d4b3e-fdb2-4cdd-abe9-a60cda085eeb",
   "metadata": {},
   "source": [
    "## main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb9615e-45c8-4d95-8a1e-367ad3e8576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from icecream import ic\n",
    "\n",
    "from data.dataloader import (\n",
    "    get_sentiment_from_rating, \n",
    "    test_data\n",
    ")\n",
    "from evaluate.metrics import (\n",
    "    plot_and_save_confusion_matrix,\n",
    "    evaluate_model\n",
    ")\n",
    "from results import results_dir_path\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"LiYuan/amazon-review-sentiment-analysis\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"LiYuan/amazon-review-sentiment-analysis\")\n",
    "\n",
    "def predict_sentiment(review:str)->str:\n",
    "\n",
    "    try:\n",
    "        inputs = tokenizer(review, return_tensors=\"pt\",truncation=True)\n",
    "\n",
    "        # Make prediction\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        prediction_index = torch.argmax(logits, dim=1)\n",
    "        sentiment = get_sentiment_from_rating(prediction_index.item())\n",
    "\n",
    "        return sentiment\n",
    "\n",
    "    except Exception as e:\n",
    "        ic(review)\n",
    "        ic(e)\n",
    "\n",
    "        return \"neutral\"\n",
    "\n",
    "save_file_path = os.path.join(\n",
    "    results_dir_path,\n",
    "    \"bert\"\n",
    ")\n",
    "\n",
    "test_data['prediction'] = test_data['reviewText'].progress_apply(predict_sentiment)\n",
    "csv_file_path = os.path.join(save_file_path,\"_data.csv\")\n",
    "test_data.to_csv(csv_file_path, index=False)\n",
    "\n",
    "prediction = list(test_data['prediction'])\n",
    "actual = list(test_data['sentiment'])\n",
    "\n",
    "evaluate_model(\n",
    "    actual=actual,\n",
    "    prediction=prediction,\n",
    "    save_file_path=save_file_path,\n",
    "    file_name=\"evaluation_matrix\"\n",
    ")\n",
    "\n",
    "plot_and_save_confusion_matrix(\n",
    "    actual=actual,\n",
    "    predicted=prediction,\n",
    "    save_file_path=save_file_path,\n",
    "    file_name=\"confusion_matrix\"\n",
    ")\n",
    "\n",
    "csv_file_path = os.path.join(save_file_path,\"data.csv\")\n",
    "test_data.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c59e55-c20e-43da-b11a-8e81b335ab42",
   "metadata": {},
   "source": [
    "# Vader module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0fa9db-b3e4-4293-9372-0b19044573fe",
   "metadata": {},
   "source": [
    "## main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d1ccdb-67c0-4071-8ae2-413fdee4c38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from icecream import ic\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "from data.dataloader import test_data\n",
    "from evaluate.metrics import evaluate_model, plot_and_save_confusion_matrix\n",
    "from results import results_dir_path\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Download WordNet lemmatizer data\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Download Punkt tokenizer models\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Download the VADER lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "def preprocess_text(text):\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stop words\n",
    "    filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "\n",
    "    # Join the tokens back into a string\n",
    "    processed_text = ' '.join(lemmatized_tokens)\n",
    "\n",
    "    return processed_text\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def analyze_sentiment(review_text:str):\n",
    "\n",
    "    score = sia.polarity_scores(review_text)\n",
    "    if score['compound'] >= 0.05:\n",
    "        return 'positive'\n",
    "    elif score['compound'] <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "test_data['prediction'] = test_data['reviewText'].progress_apply(analyze_sentiment)\n",
    "\n",
    "prediction = list(test_data['prediction'])\n",
    "actual = list(test_data['sentiment'])\n",
    "save_file_path = os.path.join(\n",
    "    results_dir_path,\n",
    "    \"vader\"\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    actual=actual,\n",
    "    prediction=prediction,\n",
    "    save_file_path=save_file_path,\n",
    "    file_name=\"evaluation_matrix\"\n",
    ")\n",
    "\n",
    "plot_and_save_confusion_matrix(\n",
    "    actual=actual,\n",
    "    predicted=prediction,\n",
    "    save_file_path=save_file_path,\n",
    "    file_name=\"confusion_matrix\"\n",
    ")\n",
    "\n",
    "test_data['processedReviewText'] = test_data['reviewText'].progress_apply(preprocess_text)\n",
    "test_data['predictionProcessed'] = test_data['processedReviewText'].progress_apply(analyze_sentiment)\n",
    "\n",
    "prediction = list(test_data['predictionProcessed'])\n",
    "save_file_path = os.path.join(\n",
    "    results_dir_path,\n",
    "    \"vader\"\n",
    ")\n",
    "\n",
    "evaluate_model(\n",
    "    actual=actual,\n",
    "    prediction=prediction,\n",
    "    save_file_path=save_file_path,\n",
    "    file_name=\"evaluation_matrix_processed\"\n",
    ")\n",
    "\n",
    "plot_and_save_confusion_matrix(\n",
    "    actual=actual,\n",
    "    predicted=prediction,\n",
    "    save_file_path=save_file_path,\n",
    "    file_name=\"confusion_matrix_processed\"\n",
    ")\n",
    "\n",
    "csv_file_path = os.path.join(save_file_path,\"data.csv\")\n",
    "test_data.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14fef5c-7ee2-4cc3-9081-87180896c401",
   "metadata": {},
   "source": [
    "# llm module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2582f9bb-b0f5-4298-b2f3-0e884d9daa2a",
   "metadata": {},
   "source": [
    "## embedding.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e8807-9714-4545-a8af-834e8cfd36eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import GooglePalmEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "def embedding_function():\n",
    "    \n",
    "    # Initialize GooglePalmEmbeddings\n",
    "    embeddings = GooglePalmEmbeddings()\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadc4fd9-4128-4dc4-aeeb-13b1c4ddb27d",
   "metadata": {},
   "source": [
    "## prompts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698051fa-c6a2-498c-b398-9236bbaabe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis_prompt = \"\"\"\n",
    "    Analyze the sentiment of the following customer review. \n",
    "    Note that the review may contain words that typically have a sensitive connotation, \n",
    "    but here they are used in the context of describing clothing or fashion items. \n",
    "    Your task is to interpret these words correctly within this context and \n",
    "    determine the overall sentiment of the review - \n",
    "    whether it is positive, negative, or neutral. \n",
    "    Please provide a clear sentiment label (positive/negative/neutral), focusing solely on the customer's \n",
    "    satisfaction or dissatisfaction with the clothing item. Provide the sentiment label only.\n",
    "    Review: {review}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9832b6c1-65f5-40de-bcc5-9642ada8ba8f",
   "metadata": {},
   "source": [
    "## prompt_analyser.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d4b14-a430-422d-aafc-2f47a392fe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import GooglePalm\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from dotenv import load_dotenv\n",
    "from icecream import ic\n",
    "from langchain.llms import OpenAI\n",
    "import json\n",
    "from pydantic import BaseModel, validator\n",
    "\n",
    "from llm.prompts import sentiment_analysis_prompt\n",
    "\n",
    "class SentimentModel(BaseModel):\n",
    "    sentiment: str\n",
    "\n",
    "    @validator('sentiment')\n",
    "    def match_sentiment(cls, v):\n",
    "        allowed_values = [\"positive\", \"negative\", \"neutral\"]\n",
    "        matched_value = next((val for val in allowed_values if val in v.lower()), None)\n",
    "        return matched_value\n",
    "\n",
    "# Example Usage\n",
    "model = SentimentModel(sentiment=\"I feel very positive about this!\")\n",
    "print(model.sentiment)  # Output will be \"positive\" if it's found in the input string\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = GooglePalm(temperature=0.0)\n",
    "chat_llm = ChatGoogleGenerativeAI(model=\"gemini-pro\")\n",
    "\n",
    "# gpt_llm = OpenAI(model_name=\"text-davinci-003\", temperature=0.0)\n",
    "\n",
    "def predict_sentiment(review:str):\n",
    "\n",
    "    try:\n",
    "\n",
    "        prompt = PromptTemplate(\n",
    "            template=sentiment_analysis_prompt,\n",
    "            input_variables=[\"review\"],\n",
    "        )\n",
    "\n",
    "        chain = prompt | llm \n",
    "        result = chain.invoke({\n",
    "                \"review\" : review,\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        ic(\"In exception\")\n",
    "        prompt = \"\"\"\n",
    "            Analyze the sentiment of the following customer review. \n",
    "            Note that the review may contain words that typically have a sensitive connotation, \n",
    "            but here they are used in the context of describing clothing or fashion items. \n",
    "            Your task is to interpret these words correctly within this context and \n",
    "            determine the overall sentiment of the review - \n",
    "            whether it is positive, negative, or neutral. \n",
    "            Please provide a clear sentiment label (positive/negative/neutral), focusing solely on the customer's \n",
    "            satisfaction or dissatisfaction with the clothing item. Provide the sentiment label only.\n",
    "\n",
    "            Review: {review}\n",
    "        \"\"\"\n",
    "        output = chat_llm.invoke(prompt).content.lower()\n",
    "        result = SentimentModel(sentiment=output).sentiment\n",
    "        ic(output)\n",
    "        ic(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from icecream import ic\n",
    "\n",
    "    review = \"\"\"\n",
    "        'Size ordered fits as expected. Looked real nice when received.  After a week '\n",
    "             'of wearing it its pretty scratched up. Scratches real easy. Very light an '\n",
    "             'almost plastic feeling. But hey its a 14 dollar ring. Over all i like it' \n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "        Please analyze the sentiment of the given fashion product review and \n",
    "        classify it as either positive, negative, or neutral. \n",
    "        Please provide a detailed response that accurately represents the user's sentiment. \n",
    "        Provide the answer in a single word.\n",
    "\n",
    "        Product Review: {review}\n",
    "\n",
    "        Sentiment:\n",
    "\n",
    "    \"\"\"\n",
    "    ic(google_llm.invoke(prompt))\n",
    "\n",
    "    prompt = PromptTemplate(\n",
    "        template=sentiment_analysis_prompt,\n",
    "        input_variables=[\"review\"],\n",
    "    )\n",
    "\n",
    "    chain = prompt | google_llm \n",
    "    result = chain.invoke({\n",
    "        \"review\" : review,\n",
    "    })\n",
    "    ic(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef574d68-ee47-4e58-976f-a6f198d82d39",
   "metadata": {},
   "source": [
    "## main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae52da5-b86d-47cd-a578-3958464aa94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "from data.dataloader import test_data\n",
    "from evaluate.metrics import evaluate_model, plot_and_save_confusion_matrix\n",
    "from results import results_dir_path\n",
    "from llm.prompt_analyser import predict_sentiment\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "test_data['prediction'] = test_data['reviewText'].progress_apply(predict_sentiment)\n",
    "\n",
    "\n",
    "prediction = list(test_data['prediction'])\n",
    "actual = list(test_data['sentiment'])\n",
    "save_file_path = os.path.join(\n",
    "    results_dir_path,\n",
    "    \"llm\"\n",
    ")\n",
    "\n",
    "csv_file_path = os.path.join(save_file_path,\"_data.csv\")\n",
    "test_data.to_csv(csv_file_path, index=False)\n",
    "\n",
    "evaluate_model(\n",
    "    actual=actual,\n",
    "    prediction=prediction,\n",
    "    save_file_path=save_file_path,\n",
    "    file_name=\"evaluation_matrix\"\n",
    ")\n",
    "\n",
    "plot_and_save_confusion_matrix(\n",
    "    actual=actual,\n",
    "    predicted=prediction,\n",
    "    save_file_path=save_file_path,\n",
    "    file_name=\"confusion_matrix\"\n",
    ")\n",
    "\n",
    "csv_file_path = os.path.join(save_file_path,\"data.csv\")\n",
    "test_data.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb8c458-80f2-4d42-b9a6-4d33d148d020",
   "metadata": {},
   "source": [
    "# webapp module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7d12c0-baab-4931-84ad-f5abc2b1aee0",
   "metadata": {},
   "source": [
    "## main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc6338b-c9c0-4ff1-a156-147bf464bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from llm.prompt_analyser import predict_sentiment\n",
    "\n",
    "# Streamlit UI elements\n",
    "st.title(\"Product Review Sentiment Analysis\")\n",
    "review_text = st.text_area(\"Enter your product review here:\")\n",
    "predict_button = st.button(\"Predict Sentiment\")\n",
    "\n",
    "# Perform sentiment analysis when the button is clicked\n",
    "if predict_button:\n",
    "    if review_text.strip() == \"\":\n",
    "        st.error(\"Please enter a review before predicting sentiment.\")\n",
    "    else:\n",
    "        sentiment = predict_sentiment(review_text)\n",
    "        st.header(f\"Sentiment: {sentiment}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
